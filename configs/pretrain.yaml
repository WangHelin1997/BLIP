# dataset_name: "westbrook/gigaspeech-tiny-stage4"
# configuration: "default"  
pretrain_cache_dir: "/data/lmorove1/hwang258/sc/Speech-Captioning-Dataset/cache/out/gigaspeech-tiny-train"
fea_dir: "/data/lmorove1/hwang258/dataspeech/hubert_features"

# size of vit model; base or large
input_dim: 25600
hidden_dim: 512
kernel_size: 5
padding: 2
pooling: 5
dropout: 0.4
audio_width: 512

batch_size: 75

queue_size: 57600
alpha: 0.4

# optimizer
weight_decay: 0.05
init_lr: 3e-4
min_lr: 1e-6
warmup_lr: 1e-6
lr_decay_rate: 0.9
max_epoch: 20
warmup_steps: 3000



